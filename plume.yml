x-logging: &logging
  logging:
    driver: json-file
    options:
      max-size: 100m
      max-file: "3"
      tag: '{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}'

services:
  snapshot-init:
    image: alpine:3.19
    restart: "no"
    environment:
      - SNAPSHOT_URL=${SNAPSHOT_URL:-}
    volumes:
      - plume-data:/home/user/.arbitrum
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        set -euo pipefail
        DATA_DIR=/home/user/.arbitrum/conduit-orbit-deployer/nitro
        MARKER_FILE=.snapshot_restored
        TUID=1000
        TGID=1000
        # Internal marker path (define before use) - use $$ so docker compose doesn't try to interpolate
        MARKER="$$DATA_DIR/$$MARKER_FILE"
        if [ -z "${SNAPSHOT_URL:-}" ]; then
          echo "[snapshot-init] SNAPSHOT_URL not set -> nothing to do"; exit 0; fi
        # Install needed tools
        apk add --no-cache aria2 curl tar lz4 zstd >/dev/null 2>&1 || true
        # Only restore if marker not present
        if [ -f "$$MARKER" ]; then
          echo "[snapshot-init] Marker present ($$MARKER) -> skipping"; exit 0; fi
        # Ensure target data directory exists
        mkdir -p "$$DATA_DIR"
        # Safety check: if directory is non-empty and no marker, refuse to overwrite to avoid corrupting an existing sync
        if [ -n "$(ls -A "$$DATA_DIR" 2>/dev/null)" ]; then
          echo "[snapshot-init] WARNING: DATA_DIR ($$DATA_DIR) not empty and no marker present. Refusing to apply snapshot.";
          echo "[snapshot-init] Move or remove existing data, then re-run, if you intended to restore a snapshot.";
          exit 1;
        fi
        echo "[snapshot-init] Restoring snapshot from $SNAPSHOT_URL"
        TMP_ARCHIVE=/tmp/snapshot.archive
        # Prefer aria2c for faster multi-connection download, fallback to curl
        if command -v aria2c >/dev/null 2>&1; then
          echo "[snapshot-init] Using aria2c for download"
          aria2c -x 16 -s 16 -k 4M --timeout=30 --retry-wait=5 --max-tries=5 -o "$(basename "$$TMP_ARCHIVE")" -d /tmp "$SNAPSHOT_URL" || { echo "[snapshot-init] aria2c failed, falling back to curl"; curl -L --fail --retry 3 --retry-delay 5 "$SNAPSHOT_URL" -o "$$TMP_ARCHIVE"; }
        else
          curl -L --fail --retry 3 --retry-delay 5 "$SNAPSHOT_URL" -o "$$TMP_ARCHIVE"
        fi
        # Detect by content / extension
        case "$SNAPSHOT_URL" in
          *.tar.lz4|*.lz4) echo "[snapshot-init] Detected lz4 archive"; lz4 -d "$$TMP_ARCHIVE" -c | tar -x -C "$$DATA_DIR" ;;
          *.tar.zst|*.zst) echo "[snapshot-init] Detected zstd archive"; zstd -d "$$TMP_ARCHIVE" -c | tar -x -C "$$DATA_DIR" ;;
          *.tar.gz|*.tgz|*.gz) echo "[snapshot-init] Detected gzip archive"; tar -xzf "$$TMP_ARCHIVE" -C "$$DATA_DIR" ;;
          *.tar) echo "[snapshot-init] Detected tar archive"; tar -xf "$$TMP_ARCHIVE" -C "$$DATA_DIR" ;;
          *) echo "[snapshot-init] Unknown extension -> attempting generic tar extract"; tar -xf "$$TMP_ARCHIVE" -C "$$DATA_DIR" || { echo "[snapshot-init] Extraction failed"; exit 1; };;
        esac
        # Cleanup archive to free space
        rm -f "$$TMP_ARCHIVE"
        touch "$$MARKER"
        # Adjust ownership so the runtime user in consensus container can read/write
        if command -v chown >/dev/null 2>&1; then
          echo "[snapshot-init] Setting ownership to $$TUID:$$TGID";
          chown -R $$TUID:$$TGID "$$DATA_DIR/.." || echo "[snapshot-init] WARNING: chown failed";
        fi
        echo "[snapshot-init] Snapshot restored successfully (marker created)"
        # List top-level contents for visibility
        ls -1 "$$DATA_DIR" | sed 's/^/[snapshot-init] restored: /'
        exit 0

  celestia-light:
    image: ghcr.io/celestiaorg/celestia-node:v0.25.3
    restart: unless-stopped
    <<: *logging
    environment:
      - CELESTIA_NETWORK=${CELESTIA_NETWORK:-mainnet}
    volumes:
      - celestia-light-data:/home/celestia
    command:
      - >
        sh -c "
        if [ ! -d /home/celestia/.celestia-light ]; then
          echo '[celestia-light] First run -> init';
          celestia light init --p2p.network ${CELESTIA_NETWORK:-mainnet};
        fi;
        echo '[celestia-light] Starting light node on network ${CELESTIA_NETWORK:-mainnet}';
        exec celestia light start --p2p.network ${CELESTIA_NETWORK:-mainnet}
        "

  daprovider:
    image: ghcr.io/celestiaorg/nitro:v3.6.8
    restart: unless-stopped
    <<: *logging
    entrypoint: /usr/local/bin/daprovider
    command:
      - --das-server.addr
      - "0.0.0.0"
      - --das-server.port
      - "9880"
      - --das-server.data-availability.enable
      - --das-server.data-availability.rest-aggregator.enable
      - --das-server.data-availability.rest-aggregator.urls
      - "https://das-plume-mainnet-1.t.conduit.xyz"
      - --das-server.data-availability.parent-chain-node-url
      - "${ETH_RPC_URL:-}"
      - --das-server.data-availability.sequencer-inbox-address
      - "0x85eC1b9138a8b9659A51e2b51bb0861901040b59"

  celestia-server:
    image: ghcr.io/celestiaorg/nitro-das-celestia:v0.5.4
    restart: unless-stopped
    <<: *logging
    depends_on:
      - daprovider
      - celestia-light
    entrypoint:
      - /bin/celestia-server
      - --das.enable
      - --fallback-enabled
      - --celestia.namespace-id
      - "00000d048007a33abfeb"
      - --rpc-addr
      - "0.0.0.0"
      - --rpc-port
      - "${CELESTIA_NODE_RPC_PORT:-26657}"
      - --das.rpc.url
      - "http://daprovider:9880"
      - --celestia.rpc
      - "${CELESTIA_RPC:-http://celestia-light:26657}" # default to internal light node
      - --log-level
      - "DEBUG"

  consensus:
    image: ghcr.io/celestiaorg/nitro:v3.6.8
    restart: unless-stopped
    environment:
      - ETH_RPC_URL=${ETH_RPC_URL:-}
      - ETH_BEACON_RPC_URL=${ETH_BEACON_RPC_URL:-}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - EXTRA_FLAGS=${EXTRA_FLAGS:-}
    ports:
      - ${P2P_PORT:-30303}:30303/tcp
      - ${P2P_PORT:-30303}:30303/udp
    <<: *logging
    volumes:
      - plume-data:/home/user/.arbitrum
    depends_on:
      snapshot-init:
        condition: service_completed_successfully
      celestia-server:
        condition: service_started
    entrypoint:
      - /usr/local/bin/nitro
      - --validation.wasm.allowed-wasm-module-roots
      - /home/user/nitro-legacy/machines,/home/user/target/machines
      - --chain.id=98866
      - --chain.name=conduit-orbit-deployer
      - --http.addr=0.0.0.0
      - --http.corsdomain=*
      - --http.vhosts=*
      - --http.port=${RPC_PORT:-8545}
      - --http.api=net,web3,eth,txpool,debug,admin,arb,arbdebug,arbtrace
      - --ws.expose-all
      - --ws.rpcprefix=/
      - --ws.port=${WS_PORT:-8546}
      - --ws.addr=0.0.0.0
      - --ws.origins=*
      - --ws.api=net,web3,eth,txpool,debug
      - "--chain.info-json=[{\"chain-id\":98866,\"parent-chain-id\":1,\"chain-name\":\"conduit-orbit-deployer\",\"chain-config\":{\"chainId\":98866,\"homesteadBlock\":0,\"daoForkBlock\":null,\"daoForkSupport\":true,\"eip150Block\":0,\"eip150Hash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\",\"eip155Block\":0,\"eip158Block\":0,\"byzantiumBlock\":0,\"constantinopleBlock\":0,\"petersburgBlock\":0,\"istanbulBlock\":0,\"muirGlacierBlock\":0,\"berlinBlock\":0,\"londonBlock\":0,\"clique\":{\"period\":0,\"epoch\":0},\"arbitrum\":{\"EnableArbOS\":true,\"AllowDebugPrecompiles\":false,\"DataAvailabilityCommittee\":true,\"InitialArbOSVersion\":32,\"InitialChainOwner\":\"0x5Ec32984332eaB190cA431545664320259D755d8\",\"GenesisBlockNum\":0}},\"rollup\":{\"bridge\":\"0x35381f63091926750F43b2A7401B083263aDEF83\",\"inbox\":\"0x943fc691242291B74B105e8D19bd9E5DC2fcBa1D\",\"sequencer-inbox\":\"0x85eC1b9138a8b9659A51e2b51bb0861901040b59\",\"rollup\":\"0x35c60Cc77b0A8bf6F938B11bd3E9D319a876c2aC\",\"validator-utils\":\"0x84eA2523b271029FFAeB58fc6E6F1435a280db44\",\"validator-wallet-creator\":\"0x0A5eC2286bB15893d5b8f320aAbc823B2186BA09\",\"deployed-at\":21887008}}]"
      - --node.da-provider.enable=true
      - --node.da-provider.rpc.url=http://celestia-server:${CELESTIA_NODE_RPC_PORT:-26657}
      - --node.da-provider.with-writer=true
      - --node.data-availability.enable=false
      - --node.data-availability.rest-aggregator.enable=true
      - --node.data-availability.rest-aggregator.urls=https://das-plume-mainnet-1.t.conduit.xyz
      - --execution.forwarding-target=https://rpc.plume.org
      - --execution.caching.archive
      - --parent-chain.connection.url=${ETH_RPC_URL:-}
      - --parent-chain.blob-client.beacon-url=${ETH_BEACON_RPC_URL:-}
      - --node.staker.enable=false
      - --node.feed.input.url=wss://relay-plume-mainnet-1.t.conduit.xyz
      - --node.sequencer=false
      - --execution.rpc.tx-fee-cap=100
      - --execution.rpc.gas-cap=500000000
      - --metrics
      - --metrics-server.addr=0.0.0.0
      - --metrics-server.port=${METRICS_PORT:-6070}
      - --metrics-server.update-interval=5s
    labels:
      - traefik.enable=true
      # RPC Router
      - traefik.http.routers.${RPC_HOST:-plume-rpc}.service=${RPC_HOST:-plume-rpc}
      - traefik.http.routers.${RPC_HOST:-plume-rpc}.entrypoints=websecure
      - traefik.http.routers.${RPC_HOST:-plume-rpc}.rule=Host(`${RPC_HOST:-plume-rpc}.${DOMAIN}`)
      - traefik.http.routers.${RPC_HOST:-plume-rpc}.tls.certresolver=letsencrypt
      - traefik.http.routers.${RPC_HOST:-plume-rpc}lb.service=${RPC_HOST:-plume-rpc}
      - traefik.http.routers.${RPC_HOST:-plume-rpc}lb.entrypoints=websecure
      - traefik.http.routers.${RPC_HOST:-plume-rpc}lb.rule=Host(`${RPC_LB:-plume-lb}.${DOMAIN}`)
      - traefik.http.routers.${RPC_HOST:-plume-rpc}lb.tls.certresolver=letsencrypt
      - traefik.http.services.${RPC_HOST:-plume-rpc}.loadbalancer.server.port=${RPC_PORT:-8545}
      # WS Router
      - traefik.http.routers.${WS_HOST:-plume-ws}.service=${WS_HOST:-plume-ws}
      - traefik.http.routers.${WS_HOST:-plume-ws}.entrypoints=websecure
      - traefik.http.routers.${WS_HOST:-plume-ws}.rule=Host(`${WS_HOST:-plume-ws}.${DOMAIN}`)
      - traefik.http.routers.${WS_HOST:-plume-ws}.tls.certresolver=letsencrypt
      - traefik.http.routers.${WS_HOST:-plume-ws}lb.service=${WS_HOST:-plume-ws}
      - traefik.http.routers.${WS_HOST:-plume-ws}lb.entrypoints=websecure
      - traefik.http.routers.${WS_HOST:-plume-ws}lb.rule=Host(`${WS_LB:-plume-ws-lb}.${DOMAIN}`)
      - traefik.http.routers.${WS_HOST:-plume-ws}lb.tls.certresolver=letsencrypt
      - traefik.http.services.${WS_HOST:-plume-ws}.loadbalancer.server.port=${WS_PORT:-8546}

volumes:
  plume-data:
  celestia-light-data:
